mutate(text_no_pos = str_replace_all(text_pos, "/NNG", ""))
ds_noun_df %>%
count(source)
ds_noun_df %>%
filter(source %in% c("국제뉴스", "전자신문"))
ds_noun_df %>%
filter(source %in% c("국제뉴스", "전자신문")) %>%
count(text_no_pos)
ds_noun_df %>%
filter(source %in% c("국제뉴스", "전자신문")) %>%
count(text_no_pos, sort=TRUE)
ds_noun_df %>%
filter(source %in% c("국제뉴스", "전자신문")) %>%
count(text_no_pos, sort=TRUE)
ds_noun_df %>%
filter(source %in% c("국제뉴스", "전자신문")) %>%
count(source, text_no_pos, sort=TRUE)
ds_noun_df %>%
count(source, text_no_pos, sort=TRUE)
# Chunk 1
# source("tools/chunk-options.R")
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE,
comment="", digits = 3, tidy = FALSE, prompt = TRUE, fig.align = 'center')
library(here)
# Chunk 2: naver-news-crawler
# 0. 환경설정 -----
library(tidyverse)
library(rvest)
library(glue)
# 1. 데이터 긁어오기 -----
## 네이버 뉴스 함수 작성
get_naver_news <- function(keyword, page){
url <- glue("https://search.naver.com/search.naver?&where=news&query=",keyword,"&sm=tab_pge&sort=0&photo=0&field=0&reporter_article=&pd=1&ds=2018.02.25&de=2018.03.04&docid=&nso=so:r,p:1w,a:all&mynews=0&cluster_rank=38&start=", page,"&refresh_start=0")
news_html <- read_html(url, handle = curl::new_handle("useragent" = "Mozilla/5.0"))
title     <- html_nodes(news_html, xpath='//*[@class="type01"]/li/dl/dt/a') %>% html_text
source   <- html_nodes(news_html, xpath='//*[@class="type01"]/li/dl/dd[1]/span[1]') %>% html_text
text  <- html_nodes(news_html, xpath='//*[@class="type01"]/li/dl/dd[2]') %>%  html_text
news_df <- data.frame(title, source, text)
return(news_df)
}
page_list <- 1:10
keyword_list <- rep("빅데이터", 10)
ds_df <- map2_df(keyword_list, page_list, get_naver_news)
DT::datatable(ds_df)
# Chunk 3: naver-news-preprocessing
# KoSpacing 관련 팩키지 설치
# library(installr) # install.packages('installr')
# install.conda()
# reticulate::conda_version()
# reticulate::conda_list()
# devtools::install_github('haven-jeon/KoSpacing')
library(KoSpacing)
ds_df$text <- spacing(ds_df$text) %>% unlist
# Chunk 5: naver-news-preprocessing-morpheme
# install.packages("RcppMeCab")
library(RmecabKo)
ds_df <- ds_df %>%
mutate(clean_text = str_replace_all(text, "[[:punct:]]", "")) %>%
tbl_df
# Chunk 6: naver-news-preprocessing-morpheme-pos
ds_df <- ds_df %>%
mutate(text_pos = pos(clean_text))
ds_noun_df <- ds_df %>%
select(source, text_pos) %>%
unnest(text_pos) %>%
filter(str_detect(text_pos, "/NNG")) %>%
mutate(text_no_pos = str_replace_all(text_pos, "/NNG", ""))
ds_noun_df %>%
count(source, text_no_pos, sort=TRUE)
ds_noun_df %>%
count(text_no_pos, sort=TRUE)
ds_noun_df %>%
count(text_no_pos, sort=TRUE)  %>%
filter(!text_no_pos %in% c("데이터", "빅", "빅데이터")) %>%
ggplot(aes(x=text_no_pos, y=n)) +
geom_col()
ds_noun_df %>%
count(text_no_pos, sort=TRUE)  %>%
filter(!text_no_pos %in% c("데이터", "빅", "빅데이터")) %>%
top_n(15, weight=n) %>%
ggplot(aes(x=text_no_pos, y=n)) +
geom_col()
ds_noun_df %>%
count(text_no_pos, sort=TRUE)  %>%
filter(!text_no_pos %in% c("데이터", "빅", "빅데이터")) %>%
top_n(15) %>%
ggplot(aes(x=text_no_pos, y=n)) +
geom_col()
ds_noun_df %>%
count(text_no_pos, sort=TRUE)  %>%
filter(!text_no_pos %in% c("데이터", "빅", "빅데이터")) %>%
top_n(15) %>%
ggplot(aes(x=fct_reorder(text_no_pos, n), y=n)) +
geom_col() +
theme_bw()
ds_noun_df %>%
count(text_no_pos, sort=TRUE)  %>%
filter(!text_no_pos %in% c("데이터", "빅", "빅데이터")) %>%
top_n(15) %>%
ggplot(aes(x=fct_reorder(text_no_pos, n), y=n)) +
geom_col() +
theme_bw() +
coord_flip()
ds_df %>%
count(source)
ds_df %>%
count(source) %>%
arrange(desc(n))
ds_noun_df
ds_noun_df %>%
filter(source %in% c("블록체인밸리", "연합뉴스")) %>%
count(source, text_no_pos, sort=TRUE)
ds_noun_df %>%
filter(source %in% c("블록체인밸리", "연합뉴스")) %>%
count(source, text_no_pos, sort=TRUE)  %>%
filter(!text_no_pos %in% c("데이터", "빅", "빅데이터")) %>%
spread(source, n)
ds_noun_df %>%
filter(source %in% c("블록체인밸리", "연합뉴스")) %>%
count(source, text_no_pos, sort=TRUE)  %>%
filter(!text_no_pos %in% c("데이터", "빅", "빅데이터")) %>%
spread(source, n, fill =0)
ds_noun_df %>%
filter(source %in% c("블록체인밸리", "연합뉴스")) %>%
count(source, text_no_pos, sort=TRUE)  %>%
filter(!text_no_pos %in% c("데이터", "빅", "빅데이터")) %>%
spread(source, n, fill =0) %>%
mutate(차이 = abs(블록체인밸리 - 연합뉴스))
two_source_top_df <- ds_noun_df %>%
filter(source %in% c("블록체인밸리", "연합뉴스")) %>%
count(source, text_no_pos, sort=TRUE)  %>%
filter(!text_no_pos %in% c("데이터", "빅", "빅데이터")) %>%
spread(source, n, fill =0) %>%
mutate(차이 = abs(블록체인밸리 - 연합뉴스)) %>%
top_n(15, 차이)
library(plotrix)
top_df <- ds_noun_df %>%
filter(source %in% c("블록체인밸리", "연합뉴스")) %>%
count(source, text_no_pos, sort=TRUE)  %>%
filter(!text_no_pos %in% c("데이터", "빅", "빅데이터")) %>%
spread(source, n, fill =0) %>%
mutate(`차이` = abs(`블록체인밸리` - `연합뉴스`)) %>%
top_n(15, `차이`)
pyramid.plot(top_df$블록체인밸리, top_df$연합뉴스,
labels = top_df$차이,
gap = 12,
top.labels = c("블록체인밸리", "공통어", "연합뉴스"),
main = "빅데이터기사 공통사용 단어", unit = NULL)
top_df
pyramid.plot(top_df$블록체인밸리, top_df$연합뉴스,
labels = top_df$text_no_pos,
gap = 12,
top.labels = c("블록체인밸리", "공통어", "연합뉴스"),
main = "빅데이터기사 공통사용 단어", unit = NULL)
install.packages("dslabs")
install.packages("plotly")
install.packages("crosstalk")
install.packages("leaflet")
install.packages("gapminder")
install.packages("GGally")
library(reticulate)
conda_list()
use_condaenv("r-conda")
# Python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.externals import joblib
repl_python()
import numpy as np
exit
# Python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.externals import joblib
repl_python()
# Python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.externals import joblib
# Python
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn import preprocessing
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import make_pipeline
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.externals import joblib
library(reticulate)
conda_list()
use_condaenv("r-conda")
use_condaenv("ANACON~1")
repl_python()
# 콘솔창 R에서 파이썬 변환
# reticulate::repl_python()
# KoNLPY 라이브러리
from konlpy.tag import Kkma, Hannanum, Komoran, Mecab, Twitter
# 형태소 분석기 생성자
hannanum = Hannanum()
# twitter = Twitter()
# mecab = Mecab()
# kkma = Kkma()
# komoran = Komoran()
# 문장을 형태소로 변환
morph_list = hannanum.morphs('@챗봇 내일 판매율 예측해서 Anderson한테 이메일로 보내줘.')
print(morph_list)
# 품사 POS(Part of Speech) 태그
pos_list = hannanum.pos('@챗봇 내일 판매율 예측해서 Anderson한테 이메일로 보내줘.')
print(pos_list)
# 단어추출
noun_list = hannanum.nouns('철학은 기술을 만들고 기술은 문화를 만든다')
print(noun_list)
library(babynames) # devtools::install_github("hadley/babynames")
babynames
babynames %>% tail
library(tidyverse)
babynames %>% tail
reticulate::repl_python()
repl_python()
reticulate::repl_python()
cheeses = ['Cheddar', 'Edam', 'Gouda']
cheeses.extend(['Mozzarella', 'Roquefort'])
print(cheeses)
cheeses.pop(position)
position = cheeses.index('Mozzarella')
# 치즈 리스트에서 해당 치즈 삭제
cheeses.pop(position)
print(cheeses)
quit
# Create the empty list: baby_names
baby_names = []
# Loop over records
for row in r$babynames:
# Add the name to the list
baby_names.append(row[3])
# Sort the names in alphabetical order
for name in sorted(baby_names):
# Print each name
print(name)
reticulate::repl_python()
reticulate::repl_python()
quit
# Create the empty list: baby_names
baby_names = []
# Create the empty list: baby_names
baby_names = []
reticulate::repl_python()
quit
# Create the empty list: baby_names
baby_names = []
reticulate::repl_python()
quit
# Create the empty list: baby_names
baby_names = []
reticulate::repl_python()
quit
reticulate::repl_python()
print('hll')
baby_names = []
baby_names = []
for row in r$babynames:
quit
for row in r$babynames:
# Add the name to the list
baby_names.append(row[3])
# Sort the names in alphabetical order
for name in sorted(baby_names):
# Print each name
print(name)
reticulate::repl_python()
babynames
babynames
quit
library(tidyverse)
library(babynames) # devtools::install_github("hadley/babynames")
reticulate::repl_python()
r$babynames
library(tuber)
devtools::install_github("soodoku/tuber", build_vignettes = TRUE)
library(tuber) # devtools::install_github("soodoku/tuber", build_vignettes = TRUE)
library(tuber) # devtools::install_github("soodoku/tuber", build_vignettes = TRUE)
devtools::install_github("soodoku/tuber", build_vignettes = TRUE)
library(tuber) # devtools::install_github("soodoku/tuber", build_vignettes = TRUE)
devtools::install_github("soodoku/tuber", build_vignettes = TRUE)
library(tuber) # devtools::install_github("soodoku/tuber", build_vignettes = TRUE)
install.packages("tuber")
library(tuber) # devtools::install_github("soodoku/tuber", build_vignettes = TRUE)
yt_oauth(yt_app_id, yt_app_pw)
yt_oauth(yt_app_id, yt_app_pw)
yt_oauth(yt_app_id, yt_app_pw)
yt_oauth(yt_app_id, yt_app_pw)
yt_oauth(yt_app_id, yt_app_pw)
yt_oauth(yt_app_id, yt_app_pw)
yt_oauth(yt_app_id, yt_app_pw)
yt_oauth(yt_app_id, yt_app_pw)
yt_oauth(yt_app_id, yt_app_pw)
library(tuber) # devtools::install_github("soodoku/tuber", build_vignettes = TRUE)
yt_oauth(yt_app_id, yt_app_pw)
yt_oauth(yt_app_id, yt_app_pw)
library(tidyverse)
library(rvest)
Sys.setlocale("LC_ALL", "C")
kisline_url <- "http://media.kisline.com/highlight/mainHighlight.nice?paper_stock=069080"
stock_dat <- kisline_url %>%
read_html() %>%
html_node(xpath = '//*[@id="summarytp1"]/table[1]') %>%
html_table()
Sys.setlocale("LC_ALL", "Korean")
stock_df <- stock_dat %>%
set_names(stock_dat[1,]) %>%
filter(row_number() != 1) %>%
gather(year, value, -`구분`) %>%
mutate(value = parse_number(value))
stock_df %>%
DT::datatable()
## 회사코드를 넣으면 기업정보를 반환시키는 함수
library(tictoc)
get_company_info <- possibly(function(stock_code) {
Sys.setlocale("LC_ALL", "C")
kisline_url <- paste0("http://media.kisline.com/highlight/mainHighlight.nice?paper_stock=", stock_code)
stock_dat <- kisline_url %>%
read_html() %>%
html_node(xpath = '//*[@id="summarytp0"]/table[1]') %>%
Sys.setlocale("LC_ALL", "Korean")
stock_df <- stock_dat %>%
set_names(stock_dat[1,]) %>%
filter(row_number() != 1) %>%
gather(year, value, -`구분`) %>%
mutate(value = parse_number(value))
return(stock_df)
}, otherwise = NA)
# webzen <- get_company_info("069080")
# get_company_info("293940")
company_df <- read_rds("data/stock_code_df.rds") %>%
select(`회사명`, `종목코드`, `업종`)
tic()
company_map_df <- company_df %>%
mutate(data = map_df(`종목코드`, ~get_company_info(.x)))
toc()
company_map_df %>%
write_rds("data/company_map_df.rds")
setwd("D:/docs/finance")
## 회사코드를 넣으면 기업정보를 반환시키는 함수
library(tictoc)
get_company_info <- possibly(function(stock_code) {
Sys.setlocale("LC_ALL", "C")
kisline_url <- paste0("http://media.kisline.com/highlight/mainHighlight.nice?paper_stock=", stock_code)
stock_dat <- kisline_url %>%
read_html() %>%
html_node(xpath = '//*[@id="summarytp0"]/table[1]') %>%
Sys.setlocale("LC_ALL", "Korean")
stock_df <- stock_dat %>%
set_names(stock_dat[1,]) %>%
filter(row_number() != 1) %>%
gather(year, value, -`구분`) %>%
mutate(value = parse_number(value))
return(stock_df)
}, otherwise = NA)
# webzen <- get_company_info("069080")
# get_company_info("293940")
company_df <- read_rds("data/stock_code_df.rds") %>%
select(`회사명`, `종목코드`, `업종`)
tic()
company_map_df <- company_df %>%
mutate(data = map_df(`종목코드`, ~get_company_info(.x)))
toc()
company_map_df %>%
write_rds("data/company_map_df.rds")
4965/60
for(i in company_df$종목코드) {
cat(i, ": ", company_df$종목코드[i])
}
for(i in company_df$종목코드) {
cat(i, ": ", company_df$종목코드[i])
}
for(i in seq_along(company_df$종목코드)) {
cat(i, ": ", company_df$종목코드[i])
}
for(i in seq_along(company_df$`종목코드`)) {
cat(i, ": ", company_df$종목코드[i])
}
seq_along(company_df$`종목코드`)
for(i in 1:nrow(company_df)) {
cat(i, ": ", company_df$종목코드[i])
}
nrow(company_df)
for(i in 1:nrow(company_df)) {
cat(i, ": ", company_df$`종목코드`[i])
}
stock_dat <- kisline_url %>%
read_html() %>%
html_node(xpath = '//*[@id="summarytp0"]/table[1]') %>%
Sys.setlocale("LC_ALL", "Korean")
Sys.setlocale("LC_ALL", "Korean")
for(i in 1:nrow(company_df)) {
cat(i, ": ", company_df$`종목코드`[i])
}
for(i in 1:nrow(company_df)) {
cat(i, ": ", company_df$`종목코드`[i], "\n")
}
company_list <- list()
company_list <- list()
for(i in 1:10) {
cat(i, ": ", company_df$`종목코드`[i], "\n")
Sys.setlocale("LC_ALL", "C")
kisline_url <- paste0("http://media.kisline.com/highlight/mainHighlight.nice?paper_stock=", stock_code)
stock_dat <- kisline_url %>%
read_html() %>%
html_node(xpath = '//*[@id="summarytp0"]/table[1]') %>%
Sys.setlocale("LC_ALL", "Korean")
company_list[[i]] <- stock_dat %>%
set_names(stock_dat[1,]) %>%
filter(row_number() != 1) %>%
gather(year, value, -`구분`) %>%
mutate(value = parse_number(value))
}
company_list <- list()
for(i in 1:10) {
cat(i, ": ", company_df$`종목코드`[i], "\n")
Sys.setlocale("LC_ALL", "C")
kisline_url <- paste0("http://media.kisline.com/highlight/mainHighlight.nice?paper_stock=", company_df$`종목코드`[i])
stock_dat <- kisline_url %>%
read_html() %>%
html_node(xpath = '//*[@id="summarytp0"]/table[1]') %>%
Sys.setlocale("LC_ALL", "Korean")
company_list[[i]] <- stock_dat %>%
set_names(stock_dat[1,]) %>%
filter(row_number() != 1) %>%
gather(year, value, -`구분`) %>%
mutate(value = parse_number(value))
}
company_list <- list()
for(i in 1:10) {
cat(i, ": ", company_df$`종목코드`[i], "\n")
Sys.setlocale("LC_ALL", "C")
kisline_url <- paste0("http://media.kisline.com/highlight/mainHighlight.nice?paper_stock=", company_df$`종목코드`[i])
stock_dat <- kisline_url %>%
read_html() %>%
html_node(xpath = '//*[@id="summarytp0"]/table[1]') %>%
Sys.setlocale("LC_ALL", "Korean")
company_list[[i]] <- stock_dat %>%
# set_names(stock_dat[1,]) %>%
filter(row_number() != 1) %>%
gather(year, value, -`구분`) %>%
mutate(value = parse_number(value))
}
library(tidyverse)
library(rvest)
Sys.setlocale("LC_ALL", "C")
kisline_url <- "http://media.kisline.com/highlight/mainHighlight.nice?paper_stock=069080"
stock_dat <- kisline_url %>%
read_html() %>%
html_node(xpath = '//*[@id="summarytp1"]/table[1]') %>%
html_table()
Sys.setlocale("LC_ALL", "Korean")
stock_df <- stock_dat %>%
set_names(make.names(stock_dat[1,], unique = TRUE)) %>%
filter(row_number() != 1) %>%
gather(year, value, -`구분`) %>%
mutate(value = parse_number(value))
stock_df %>%
DT::datatable()
get_company_info <- possibly(function(stock_code) {
Sys.setlocale("LC_ALL", "C")
kisline_url <- paste0("http://media.kisline.com/highlight/mainHighlight.nice?paper_stock=", stock_code)
stock_dat <- kisline_url %>%
read_html() %>%
html_node(xpath = '//*[@id="summarytp0"]/table[1]') %>%
Sys.setlocale("LC_ALL", "Korean")
stock_df <- stock_dat %>%
set_names(make.names(stock_dat[1,], unique = TRUE)) %>%
filter(row_number() != 1) %>%
gather(year, value, -`구분`) %>%
mutate(value = parse_number(value))
return(stock_df)
}, otherwise = NA)
# webzen <- get_company_info("069080")
get_company_info("293940")
Sys.setlocale("LC_ALL", "C")
kisline_url <- "http://media.kisline.com/highlight/mainHighlight.nice?paper_stock=293940"
stock_dat <- kisline_url %>%
read_html() %>%
html_node(xpath = '//*[@id="summarytp1"]/table[1]') %>%
html_table()
Sys.setlocale("LC_ALL", "Korean")
stock_df <- stock_dat %>%
set_names(make.names(stock_dat[1,], unique = TRUE)) %>%
filter(row_number() != 1) %>%
gather(year, value, -`구분`) %>%
mutate(value = parse_number(value))
stock_df %>%
DT::datatable()
stock_df
